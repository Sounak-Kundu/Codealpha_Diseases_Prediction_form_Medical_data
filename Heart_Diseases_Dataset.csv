"# -*- coding: utf-8 -*-"
"""""""Heart Disease and Some scikit-learn Magic"
""
"Automatically generated by Colab."
""
"Original file is located at"
"https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/sounakkundu66/heart-disease-and-some-scikit-learn-magic.6c2f1cda-e68a-472b-a90c-893be286e6c6.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250920/auto/storage/goog4_request%26X-Goog-Date%3D20250920T161400Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D91486a3401b0dbd7287e8b6139a285037c712f1c10874c6ad571286d416517e32e24a35da23763ae6bdd4939d544366a1464c8f15e44dc605727b3032dc3f6eaa41162cef127a8b1d9a798273e26c234ac3c00bf9e78d7112cd86ecee1ab52392de9faedc5226faa36c36ccadec67de19ebb3fce422ae8a3620a9487542cf9e07cd7a146d5b23f7a07ba635bb75173cf5ab6130cdaacfca0c50d06e11a39f01ddfa0f7282d830f3fbe8960a8e07112a4bd32417b208cc72e1b9cd63f990db9373abecbb5ea1be7d8e2106787bfcdceca5048db17a9d0db81cb98aaa6999998d6ead41ffa1118a4154e12f72dd993c1a1ae76f23335ffc852f86c0c0ac6f00515"
""""""""
""
"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,"
"# THEN FEEL FREE TO DELETE THIS CELL."
"# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON"
"# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR"
"# NOTEBOOK."
"import kagglehub"
"cherngs_heart_disease_cleveland_uci_path = kagglehub.dataset_download('cherngs/heart-disease-cleveland-uci')"
""
"print('Data source import complete.')"
""
"""""""# Introduction"
""
"Hello all!"
""
"In this notebook I tried to play with some sklearn features while exploring and visualizing the heart disease data we have given. Basically I tried to show distribution of data, relations between variables and target as well as correlations between each other then did some basic model building."
""
"The data includes 303 patient level features including if they have heart disease at the end or not. Features are like;"
""
"- Age: Obvious one..."
"- Sex:"
"- 0: Female"
"- 1: Male"
"- Chest Pain Type:"
"- 0: Typical Angina"
"- 1: Atypical Angina"
"- 2: Non-Anginal Pain"
"- 3: Asymptomatic"
"- Resting Blood Pressure: Person's resting blood pressure."
"- Cholesterol: Serum Cholesterol in mg/dl"
"- Fasting Blood Sugar:"
"- 0:Less Than 120mg/ml"
"- 1: Greater Than 120mg/ml"
"- Resting Electrocardiographic Measurement:"
"- 0: Normal"
"- 1: ST-T Wave Abnormality"
"- 2: Left Ventricular Hypertrophy"
"- Max Heart Rate Achieved: Maximum Heart Rate Achieved"
"- Exercise Induced Angina:"
"- 1: Yes"
"- 0: No"
"- ST Depression: ST depression induced by exercise relative to rest."
"- Slope: Slope of the peak exercise ST segment:"
"- 0: Upsloping"
"- 1: Flat"
"- 2: Downsloping"
"- Thalassemia: A blood disorder called 'Thalassemia':"
"- 0: Normal"
"- 1: Fixed Defect"
"- 2: Reversable Defect"
"- Number of Major Vessels: Number of major vessels colored by fluoroscopy."
""
"After doing some usual exploratory data analysis I noticed some of the results doesn't make sense, I ain't no expert in field but made me curious and then I found this topic here [The ultimate guide to this dataset!](https://www.kaggle.com/ronitf/heart-disease-uci/discussion/105877), his points made sense so I decided to use this dataset after inspecting it: [Heart Disease Cleveland UCI](https://www.kaggle.com/cherngs/heart-disease-cleveland-uci)."
""
"I'd say it gave me more reasonable results and decided to stick with it, both datasets are pretty close but targets are reversed. It's up to you what dataset you choose, the original one gave me better F1 score but as I said EDA didn't make sense to but I have to tell I'm just inspecting the data and have no medical knowledge on the field."
""
"One last note: I used some sklearn features just for the sake of showing them, they might be not needed in actual use for this case..."
""
"Well, let's get goin then!"
""
"# Loading Data"
""""""""
""
"# Loading packages."
""
"import pandas as pd"
"import numpy as np"
"import matplotlib.pyplot as plt"
"import seaborn as sns"
"import plotly.express as px"
""
"#"
""
"import matplotlib.gridspec as gridspec"
"from matplotlib.ticker import MaxNLocator"
""
"#"
""
"import math"
"import random"
"import os"
"import time"
""
"from numpy import interp"
""
"# Disabling warnings:"
""
"import warnings"
"warnings.filterwarnings('ignore')"
""
"# Styling:"
""
"cust_palt = ["
"'#111d5e', '#c70039', '#f37121', '#ffbd69', '#ffc93c'"
"]"
""
"plt.style.use('ggplot')"
""
"# Seeding:"
""
"def seed_all(seed):"
""
"''' A function to seed everything for getting stable results and reproducibility'''"
""
"random.seed(seed)"
"np.random.seed(seed)"
"os.environ['PYTHONHASHSEED'] = str(seed)"
""
"seed = 42"
"seed_all(seed)"
""
"# Reading csv file:"
""
"train = pd.read_csv(cherngs_heart_disease_cleveland_uci_path + '/heart_cleveland_upload.csv')"
""
"# Taking random samples from data:"
""
"train.head(5)"
""
"print("
"f'Train data has {train.shape[1]} features, {train.shape[0]} observations.\nTrain features are:\n{train.columns.tolist()}\n'"
")"
""
"# Checking null values:"
""
"train.isnull().sum().sum()"
""
"""""""### I decided to rename columns for easier understanding in EDA part."""""""
""
"# Renaming columns."
"train.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',"
"'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'condition']"
""
"# Number of unique train observartions:"
""
"train.nunique()"
""
"""""""### Again renaming the categorical variables for easier EDA interpretation."""""""
""
"# Renaming cateorical data for easier understanding:"
""
"train['sex'] = train['sex'].map({0:'female',1:'male'})"
""
"train['chest_pain_type'] = train['chest_pain_type'].map({3:'asymptomatic', 1:'atypical_angina', 2:'non_anginal_pain', 0:'typical_angina'})"
""
"train['fasting_blood_sugar'] = train['fasting_blood_sugar'].map({0:'less_than_120mg/ml',1:'greater_than_120mg/ml'})"
""
"train['rest_ecg'] = train['rest_ecg'].map({0:'normal',1:'ST-T_wave_abnormality',2:'left_ventricular_hypertrophy'})"
""
"train['exercise_induced_angina'] = train['exercise_induced_angina'].map({0:'no',1:'yes'})"
""
"train['st_slope'] = train['st_slope'].map({0:'upsloping',1:'flat',2:'downsloping'})"
""
"train['thalassemia'] = train['thalassemia'].map({1:'fixed_defect',0:'normal',2:'reversable_defect'})"
""
"train['condition'] = train['condition'].map({0:'no_disease', 1:'has_disease'})"
""
"# Masks for easier selection in future:"
""
"categorical = [i for i in train.loc[:,train.nunique()<=10]]"
"continuous = [i for i in train.loc[:,train.nunique()>=10]]"
""
"def ctg_dist(df, cols, hue=None,rows=3, columns=3):"
""
"'''A function for displaying cateorical distribution'''"
""
"fig, axes = plt.subplots(rows, columns, figsize=(16, 12))"
"axes = axes.flatten()"
""
"for i, j in zip(df[cols].columns, axes):"
"sns.countplot(x=i,"
"data=df,"
"palette=cust_palt,"
"hue=hue,"
"ax=j,"
"order=df[i].value_counts().index)"
"j.tick_params(labelrotation=10)"
""
"total = float(len(df[i]))"
""
"j.set_title(f'{str(i).capitalize()} Distribution')"
""
""
"for p in j.patches:"
"height = p.get_height()"
"j.text(p.get_x() + p.get_width() / 2.,"
"height + 2,"
"'{:1.2f}%'.format((height / total) * 100),"
"ha='center')"
""
"plt.tight_layout()"
""
"""""""# Univariate Analysis"
""
"> Univariate analysis is the simplest form of analyzing data. “Uni” means “one”, so in other words your data has only one variable. It doesn't deal with causes or relationships (unlike regression) and it's major purpose is to describe; It takes data, summarizes that data and finds patterns in the data."
""
"#### For this part we going to inspect how's the data distribution is and what patterns we can inspect."
""
"## Categorical Data"
""
"### Here we can do these observations:"
"- Males on the dataset is more than double of the female observations."
"- Most common ches pain type is 'Asymptomatic' ones which is almost 50% of the data"
"- 85% of the patients has no high levels of fastin blood sugar."
"- Resing electrocardiographic observations are evenly distributed between normal and left ventricular hypertrophy with ST-T minority"
"- 67% of the patients had no exercise induced angina"
"- Peak exercise slope seems mainly divided between upsloping and flat."
""""""""
""
"# Display categorical data:"
""
"ctg_dist(train, categorical)"
""
"""""""## Numerical Data"
""
"### Most of the continuous variables somewhat close to gaussian distribution with small skews left or right except for oldpeak. Again there are some outliers espacially a strong one in Cholesterol worth to take a look later."
""""""""
""
"# Displaying numeric distribution:"
""
"fig = plt.figure(constrained_layout=True, figsize=(16, 12))"
""
""
"grid = gridspec.GridSpec(ncols=6, nrows=3, figure=fig)"
""
"ax1 = fig.add_subplot(grid[0, :2])"
""
"ax1.set_title('Trestbps Distribution')"
""
"sns.distplot(train[continuous[1]],"
"hist_kws={"
"'rwidth': 0.85,"
"'edgecolor': 'black',"
"'alpha': 0.8},"
"color=cust_palt[0])"
""
"ax15 = fig.add_subplot(grid[0, 2:3])"
""
"ax15.set_title('Trestbps')"
""
"sns.boxplot(train[continuous[1]], orient='v', color=cust_palt[0])"
""
"ax2 = fig.add_subplot(grid[0, 3:5])"
""
"ax2.set_title('Chol Distribution')"
""
"sns.distplot(train[continuous[2]],"
"hist_kws={"
"'rwidth': 0.85,"
"'edgecolor': 'black',"
"'alpha': 0.8},"
"color=cust_palt[1])"
""
"ax25 = fig.add_subplot(grid[0, 5:])"
""
"ax25.set_title('Chol')"
""
"sns.boxplot(train[continuous[2]], orient='v', color=cust_palt[1])"
""
"ax3 = fig.add_subplot(grid[1, :2])"
""
"ax3.set_title('Thalach Distribution')"
""
"sns.distplot(train[continuous[3]],"
"hist_kws={"
"'rwidth': 0.85,"
"'edgecolor': 'black',"
"'alpha': 0.8},"
"color=cust_palt[2])"
""
"ax35 = fig.add_subplot(grid[1, 2:3])"
""
"ax35.set_title('Thalach')"
""
"sns.boxplot(train[continuous[3]], orient='v', color=cust_palt[2])"
""
"ax4 = fig.add_subplot(grid[1, 3:5])"
""
"ax4.set_title('Oldpeak Distribution')"
""
"sns.distplot(train[continuous[4]],"
"hist_kws={"
"'rwidth': 0.85,"
"'edgecolor': 'black',"
"'alpha': 0.8},"
"color=cust_palt[3])"
""
"ax45 = fig.add_subplot(grid[1, 5:])"
""
"ax45.set_title('Oldpeak')"
""
"sns.boxplot(train[continuous[4]], orient='v', color=cust_palt[3])"
""
"ax5 = fig.add_subplot(grid[2, :4])"
""
"ax5.set_title('Age Distribution')"
""
"sns.distplot(train[continuous[0]],"
"hist_kws={"
"'rwidth': 0.95,"
"'edgecolor': 'black',"
"'alpha': 0.8},"
"color=cust_palt[4])"
""
"ax55 = fig.add_subplot(grid[2, 4:])"
""
"ax55.set_title('Age')"
""
"sns.boxplot(train[continuous[0]], orient='h', color=cust_palt[4])"
""
"plt.show()"
""
"""""""# Bivariate Analysis"
""
"> Bivariate analysis is one of the simplest forms of quantitative analysis. It involves the analysis of two variables, for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association."
""
"In this part we goin to take our variables and compare them against our target condition which is if the observed patient has disease or not."
""
"## Categorical Data vs Target"
""
"### Here we can do these observations:"
""
"- Males are much more likely for heart diseases."
"- Chest pain type is very subjective and has no direct relation on the outcome, asymptomatic chest pains having highest disease outcome."
"- Blood sugar has no direct effect on the disease."
"- Rest ECG results showing no direct results but having normal ECG is pretty good sign. Even though it's pretty rare in the data, if you ST-T wave abnormality you are 3 times more likely to have heart disease."
"- Having exercise induced angina is pretty strong indicator for heart disease, patients are almost 3 times more likely to have disease if they have exercise induced angina. Meanwhile it's less than half for not having it."
"- Patients who had flat slope distribution are more likely to have disease."
"- Number of major vessels observed seems on similar levels for patients who have disease but 0 observations is good sign for not having disease."
"- Having defected thalium test results is pretty strong indicator for heart disease."
""""""""
""
"# Categorical data vs condition:"
""
"ctg_dist(train, categorical[:-1], 'condition', 4, 2)"
""
"""""""## Numerical Data vs Target"
""
"### Here we can do these observations:"
""
"- Having higher resting blood pressure shows you are little bit more likely to have heart disease."
"- Again same for Cholesterol, it's not strong indicator but patients are little bit more likely to have disease with high cholesterol. There's is also one outlier there with no disease, pretty interesting."
"- I find max heart rate distribution a bit interesting, expecting the other way around but it might be due to testing conditions and if you have normal results on ECG while exercising instructors might be increasing your excercise density?"
"- It's pretty clear that heart disease likelihood increases with ST depression levels..."
"- Lastly older patients are more likely to have heart disease."
""""""""
""
"# Displaying numeric distribution vs condition:"
""
"fig = plt.figure(constrained_layout=True, figsize=(16, 12))"
""
""
"grid = gridspec.GridSpec(ncols=4, nrows=3, figure=fig)"
""
"ax1 = fig.add_subplot(grid[0, :2])"
""
"ax1.set_title('resting_blood_pressure Distribution')"
""
"sns.boxplot(x='condition',"
"y='resting_blood_pressure',"
"data=train,"
"palette=cust_palt[2:],"
"ax=ax1)"
"sns.swarmplot(x='condition',"
"y='resting_blood_pressure',"
"data=train,"
"palette=cust_palt[:2],"
"ax=ax1)"
""
"ax2 = fig.add_subplot(grid[0, 2:])"
""
"ax2.set_title('cholesterol Distribution')"
""
"sns.boxplot(x='condition',"
"y='cholesterol',"
"data=train,"
"palette=cust_palt[2:],"
"ax=ax2)"
"sns.swarmplot(x='condition',"
"y='cholesterol',"
"data=train,"
"palette=cust_palt[:2],"
"ax=ax2)"
""
"ax3 = fig.add_subplot(grid[1, :2])"
""
"ax3.set_title('max_heart_rate_achieved Distribution')"
""
"sns.boxplot(x='condition',"
"y='max_heart_rate_achieved',"
"data=train,"
"palette=cust_palt[2:],"
"ax=ax3)"
"sns.swarmplot(x='condition',"
"y='max_heart_rate_achieved',"
"data=train,"
"palette=cust_palt[:2],"
"ax=ax3)"
""
"ax4 = fig.add_subplot(grid[1, 2:])"
""
"ax4.set_title('st_depression Distribution')"
""
"sns.boxplot(x='condition',"
"y='st_depression',"
"data=train,"
"palette=cust_palt[2:],"
"ax=ax4)"
"sns.swarmplot(x='condition',"
"y='st_depression',"
"data=train,"
"palette=cust_palt[:2],"
"ax=ax4)"
""
"ax5 = fig.add_subplot(grid[2, :])"
""
"ax5.set_title('age Distribution')"
""
"sns.boxplot(x='condition',"
"y='age',"
"data=train,"
"palette=cust_palt[2:],"
"ax=ax5)"
"sns.swarmplot(x='condition',"
"y='age',"
"data=train,"
"palette=cust_palt[:2],"
"ax=ax5)"
"plt.show()"
""
"""""""# Multivariate Analysis"
""
"> Multivariate analysis (MVA) is based on the principles of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time. Typically, MVA is used to address the situations where multiple measurements are made on each experimental unit and the relations among these measurements and their structures are important."
""""""""
""
"# Numeric data vs each other and condition:"
""
"plt.figure(figsize=(16, 10))"
"sns.pairplot(train[['resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression','age', 'condition']], hue='condition', palette=cust_palt,"
"markers=['o','D'], plot_kws=dict(s=25, alpha=0.75)"
")"
""
"plt.show()"
""
"""""""## Cholesterol, Max Heart Rate, Age, St Depression vs Target"
""
"### Here I tried to fit every single numerical feature into one graph so we can have some visualized version of the effects. 3D scatterplot is great tool for doing that..."
""
"#### On X axis we have Cholesterol levels, on Y Max Heart Rate presented and Z axis is patient Age, marker sizes are based on ST_Depression levels and coloring based on the patient condition."
""""""""
""
"# 3D scatterplot of numeric data:"
""
"fig = px.scatter_3d(train, x='cholesterol', y='max_heart_rate_achieved', z='age', size='st_depression',"
"color='condition', opacity=0.8)"
""
"fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))"
"fig.show()"
""
"""""""# Inspecting Age Closer"""""""
""
"def ctn_freq(df, cols, xaxi, hue=None,rows=4, columns=1):"
""
"''' A function for displaying numerical data frequency vs age and condition '''"
""
"fig, axes = plt.subplots(rows, columns, figsize=(16, 12), sharex=True)"
"axes = axes.flatten()"
""
"for i, j in zip(df[cols].columns, axes):"
"sns.pointplot(x=xaxi,"
"y=i,"
"data=df,"
"palette=cust_palt[:2],"
"hue=hue,"
"ax=j,ci=False)"
"j.set_title(f'{str(i).capitalize()} vs. Age')"
""
""
"plt.tight_layout()"
""
"ctn_freq(train, ['st_depression','max_heart_rate_achieved','resting_blood_pressure','cholesterol'], 'age', hue='condition',rows=4, columns=1)"
""
"# Loading data for corrmap:"
""
"heat_train = train.copy()"
""
"""""""# Correlations"
""
"#### We going to use pearson correlation for to find linear relations between features, heatmap is decent way to show these relations."
""""""""
""
"# Correlation heatmap between variables:"
""
"sns.set(font_scale=1.1)"
"# Create a copy to avoid modifying the original 'train' DataFrame"
"heat_train_encoded = heat_train.copy()"
""
"# One-hot encode the categorical columns"
"categorical_cols = ['sex', 'chest_pain_type', 'fasting_blood_sugar', 'rest_ecg', 'exercise_induced_angina', 'st_slope', 'num_major_vessels', 'thalassemia', 'condition']"
"heat_train_encoded = pd.get_dummies(heat_train_encoded, columns=categorical_cols, drop_first=True)"
""
""
"correlation_train = heat_train_encoded.corr()"
"mask = np.triu(correlation_train.corr())"
"plt.figure(figsize=(20, 12))"
"sns.heatmap(correlation_train,"
"annot=True,"
"fmt='.3f',"
"cmap='Wistia',"
"linewidths=1,"
"cbar=True)"
""
"plt.show()"
""
"""""""### Here I chose top related features with outcome condition, seems thal is the most correlated one..."""""""
""
"# Top correlated variables vs condition:"
""
"correlations = heat_train_encoded.corrwith(heat_train_encoded['condition_no_disease']).iloc[:-1].to_frame()"
"correlations['Abs Corr'] = correlations[0].abs()"
"sorted_correlations = correlations.sort_values('Abs Corr', ascending=False)['Abs Corr']"
"fig, ax = plt.subplots(figsize=(12,12))"
"sns.heatmap(sorted_correlations.to_frame()[sorted_correlations>=.35], cmap='Wistia', annot=True, vmin=-1, vmax=1,linewidths=1,fmt='.5f', ax=ax);"
""
"""""""# Modelling"
""
"### Ok... Here's the fun part, who doesn't love a bit of modelling? We start by loading our train data and labels as X and y's and we get dummy variables for categorical data using one hot encoding. Then we import loads of sklearn modules."
""""""""
""
"# Setting train and condition data:"
""
"X = train.drop('condition', axis=1)"
"y = train['condition']"
""
"# One hot encoding train features:"
""
"X = pd.get_dummies(data=X, columns=['sex', 'chest_pain_type', 'fasting_blood_sugar', 'rest_ecg', 'exercise_induced_angina', 'st_slope', 'num_major_vessels', 'thalassemia'])"
""
"# Loading sklearn packages:"
""
"from sklearn.model_selection import cross_validate, KFold, learning_curve,  cross_val_score, RandomizedSearchCV, train_test_split"
"from sklearn.neighbors import KNeighborsClassifier"
"from sklearn.tree import DecisionTreeClassifier"
"from sklearn.svm import SVC"
"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, IsolationForest"
"from sklearn.neural_network import MLPClassifier"
"from sklearn.naive_bayes import GaussianNB"
"from sklearn.preprocessing import KBinsDiscretizer"
"from sklearn.covariance import EllipticEnvelope"
"from sklearn.decomposition import PCA"
"from sklearn.metrics import ConfusionMatrixDisplay # Import ConfusionMatrixDisplay"
""
"""""""# Classifiers"
""
"### Here I selected some common sklearn classifiers, I didn't want to include some common ml algorithms like xgboost, lightgbm or catboost since I wanted to play with sklearn only for this notebook. I'll do small quotes for each classifier from sklearn's official page:"
""
"## GradientBoostingClassifier"
""
"> Gradient Tree Boosting or Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss functions. GBDT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems in a variety of areas including Web search ranking and ecology."
""
"## KNeighborsClassifier"
""
"> Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point."
""
"## DecisionTreeClassifier"
""
"> Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model."
""
"## Support Vector Machines"
""
"> Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection."
"- The advantages of support vector machines are:"
"- Effective in high dimensional spaces."
"- Still effective in cases where number of dimensions is greater than the number of samples."
"- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient."
"- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels."
""
"## RandomForestClassifier"
""
">The sklearn.ensemble module includes two averaging algorithms based on randomized decision trees: the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine technique specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers."
""
"## AdaBoostClassifier"
""
"> An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
""
"## MLP Classifier"
""
"> Multi-layer Perceptron classifier."
"This model optimizes the log-loss function using LBFGS or stochastic gradient descent."
""
"## GaussianNB"
""
"> GaussianNB implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian."
""
""
"### Ok... Let's get building them."
""""""""
""
"# Selecting some sklearn classifiers:"
""
"gradclass = GradientBoostingClassifier(random_state=seed)"
""
"knclass = KNeighborsClassifier()"
""
"dectree = DecisionTreeClassifier(random_state=seed)"
""
"svc = SVC()"
""
"randfclass = RandomForestClassifier(random_state=seed)"
""
"adaclass = AdaBoostClassifier(random_state=seed)"
""
"mlpclass = MLPClassifier(random_state=seed)"
""
"gsclass = GaussianNB()"
""
"# Setting 5 fold CV:"
""
"cv = KFold(5, shuffle=True, random_state=seed)"
""
"classifiers = [gradclass, knclass, dectree, svc, randfclass, adaclass, mlpclass, gsclass]"
""
"def model_check(X, y, classifiers, cv):"
""
"''' A function for testing multiple classifiers and return several metrics. '''"
""
"model_table = pd.DataFrame()"
""
"row_index = 0"
"for cls in classifiers:"
""
"MLA_name = cls.__class__.__name__"
"model_table.loc[row_index, 'Model Name'] = MLA_name"
""
"cv_results = cross_validate("
"cls,"
"X,"
"y,"
"cv=cv,"
"scoring=('accuracy','f1','roc_auc'),"
"return_train_score=True,"
"n_jobs=-1"
")"
"model_table.loc[row_index, 'Train Roc/AUC Mean'] = cv_results["
"'train_roc_auc'].mean()"
"model_table.loc[row_index, 'Test Roc/AUC Mean'] = cv_results["
"'test_roc_auc'].mean()"
"model_table.loc[row_index, 'Test Roc/AUC Std'] = cv_results['test_roc_auc'].std()"
"model_table.loc[row_index, 'Train Accuracy Mean'] = cv_results["
"'train_accuracy'].mean()"
"model_table.loc[row_index, 'Test Accuracy Mean'] = cv_results["
"'test_accuracy'].mean()"
"model_table.loc[row_index, 'Test Acc Std'] = cv_results['test_accuracy'].std()"
"model_table.loc[row_index, 'Train F1 Mean'] = cv_results["
"'train_f1'].mean()"
"model_table.loc[row_index, 'Test F1 Mean'] = cv_results["
"'test_f1'].mean()"
"model_table.loc[row_index, 'Test F1 Std'] = cv_results['test_f1'].std()"
"model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()"
""
"row_index += 1"
""
"model_table.sort_values(by=['Test F1 Mean'],"
"ascending=False,"
"inplace=True)"
""
"return model_table"
""
"""""""# Baseline Results"
""
"### We have many metrics but I decided to sort them by F1 score since precision and recall is pretty important on this case. Looking at our first results showing RandomForestClassifier is the best performing one in the list of not tuned classifiers, followed by MLP and GradientBoosting classifiers."
""
"### But we can see most of our decision tree based models are overfitting, that's something we should take a look at soon..."
""""""""
""
"# Baseline check:"
""
"raw_models = model_check(X, y, classifiers, cv)"
""
"display(raw_models)"
""
"def f_imp(classifiers, X, y, bins):"
""
"''' A function for displaying feature importances'''"
""
"fig, axes = plt.subplots(1, 2, figsize=(20, 6))"
"axes = axes.flatten()"
""
"for ax, classifier in zip(axes, classifiers):"
""
"try:"
"classifier.fit(X, y)"
"feature_imp = pd.DataFrame(sorted("
"zip(classifier.feature_importances_, X.columns)),"
"columns=['Value', 'Feature'])"
""
"sns.barplot(x=""Value"","
"y=""Feature"","
"data=feature_imp.sort_values(by=""Value"","
"ascending=False),"
"ax=ax,"
"palette='plasma')"
"plt.title('Features')"
"plt.tight_layout()"
"ax.set(title=f'{classifier.__class__.__name__} Feature Impotances')"
"ax.xaxis.set_major_locator(MaxNLocator(nbins=bins))"
"except:"
"continue"
"plt.show()"
""
"""""""## Since our decision tree based models overfitting I wanted to look which features are most effecting these decisions, I sampled two of the tree based models you can see below:"""""""
""
"# Feature importances:"
""
"f_imp([randfclass,dectree], X, y, 6)"
""
"""""""# Automatic Outlier Detection"
""
"## Before going to tune our models I decided to get rid of some outliers, we have pretty small database and we can actually remove them by hand or more basic methods. But I wanted to use what sklearn can offer us for this so we gonna try couple sklearn features."
""
"# Isolation Forest"
""
"The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature."
""
"Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node."
""
"This path length, averaged over a forest of such random trees, is a measure of normality and our decision function."
""
"Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies."
""
"Basically I set contamination rate of our data to 10% and dropped them using masks. It didn't do great on the results, we have pretty small dataset and removing some more damaging model performances probably. But it's ok for now..."
""""""""
""
"# Applying Isolation Forest:"
""
"iso = IsolationForest(contamination=0.1,random_state=seed)"
"yhat = iso.fit_predict(X)"
""
"mask = (yhat != -1)"
""
"X_iso = X.loc[mask, :]"
"y_iso= y[mask]"
""
"# Checking isolated models:"
""
"iso_models = model_check(X_iso, y_iso, classifiers, cv)"
"display(iso_models)"
""
"""""""# Elliptic Envelope"
""
"Let's try another automatic outlier detection method. We assumed our distribution close to gaussian while inspecting the data so elliptic envelope worth to take a look."
""
"> The Minimum Covariance Determinant (MCD) method is a highly robust estimator of multivariate location and scatter, for which a fast algorithm is available. […] It also serves as a convenient and efficient tool for outlier detection."
""
"This one did a little bit better than isolation forest so let's stick with it for this case..."
""""""""
""
"# Applying Elliptical Envelope:"
""
"eli = EllipticEnvelope(contamination=0.1,assume_centered=True, random_state=seed)"
"yhat = eli.fit_predict(X)"
""
"mask = (yhat != -1)"
""
"X_eli = X.loc[mask, :]"
"y_eli= y[mask]"
""
"eli_models = model_check(X_eli, y_eli, classifiers, cv)"
"display(eli_models)"
""
"""""""# Discretization"
""
"Discretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes."
""
"One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models."
""
"### Since we have small and noisy data I thougt binning them would be better choice of action, for this purpose I'm going to choose another sklearn tool:"
""
"## K-Bbins Discretization"
""
"### This method discretizes features into 'k' bins. Sklearn module takes several strategy parameters but we going to use 'kmeans' strategy which defines bins based on a k-means clustering procedure performed on each feature independently."
""""""""
""
"def kbin_cat(col, X, nbins=5):"
""
"''' A function for binning multiple numeric columns'''"
""
"categorize = KBinsDiscretizer(n_bins = nbins, encode = 'onehot', strategy = 'kmeans')"
"cat = categorize.fit_transform(X[col].values.reshape(-1,1))"
"cat= pd.DataFrame(cat.toarray())"
"cat_n = [f'cat_{str(i)}' for i in range(nbins)]"
"cat.columns = [i.replace('cat',f'{str(col)}') for i in cat_n]"
"cat = cat.astype('int')"
""
"return cat"
""
"# Applying K-bins discretizer:"
""
"rt = ['age','resting_blood_pressure','cholesterol', 'max_heart_rate_achieved','st_depression']"
"X_cat = X_eli"
"for i in rt:"
"X_cat = X_cat.join(kbin_cat(i,X,5))"
"X_cat.drop(i, axis=1, inplace=True)"
""
"""""""### By looking at the results binning improved some models little but few of them like SVC got huge boost to their score. (F1 score 0.44 > 0.80)"""""""
""
"binn_models = model_check(X_cat, y_eli, classifiers, cv)"
"display(binn_models)"
""
"""""""# Learning Curves"
""
"### Before finalizing our modelling I wanted to use another tool sklearn offers: Learning Curves. That can show us how fast the models learning and especially how is the model doing with the number of data given so we can decide if more data needed for better results. In our case we can see that some models overfitting and most of our models can get better with the more data..."
""""""""
""
"def plot_learning_curve(classifiers,"
"X,"
"y,"
"ylim=None,"
"cv=None,"
"n_jobs=None,"
"train_sizes=np.linspace(.1, 1.0, 5)):"
""
"''' A function for displaying learning curvers fur multiple ml algorithms'''"
""
"fig, axes = plt.subplots(math.ceil(len(classifiers) / 2),"
"2,"
"figsize=(25, 50))"
"axes = axes.flatten()"
""
"for ax, classifier in zip(axes, classifiers):"
""
"ax.set_title(f'{classifier.__class__.__name__} Learning Curve')"
"if ylim is not None:"
"ax.set_ylim(*ylim)"
"ax.set_xlabel('Training examples')"
"ax.set_ylabel('Score')"
""
"train_sizes, train_scores, test_scores, fit_times, _ = \"
"learning_curve(classifier, X, y, cv=cv, n_jobs=n_jobs,"
"train_sizes=train_sizes,"
"return_times=True, scoring='f1', random_state=seed"
")"
"train_scores_mean = np.mean(train_scores, axis=1)"
"train_scores_std = np.std(train_scores, axis=1)"
"test_scores_mean = np.mean(test_scores, axis=1)"
"test_scores_std = np.std(test_scores, axis=1)"
""
"# Plot learning curve"
""
"ax.fill_between(train_sizes,"
"train_scores_mean - train_scores_std,"
"train_scores_mean + train_scores_std,"
"alpha=0.1,"
"color='r')"
"ax.fill_between(train_sizes,"
"test_scores_mean - test_scores_std,"
"test_scores_mean + test_scores_std,"
"alpha=0.1,"
"color='g')"
"ax.plot(train_sizes,"
"train_scores_mean,"
"'o-',"
"color='r',"
"label='Training score')"
"ax.plot(train_sizes,"
"test_scores_mean,"
"'o-',"
"color='g',"
"label='Cross-validation score')"
"ax.legend(loc='best')"
"ax.yaxis.set_major_locator(MaxNLocator(nbins=24))"
""
"plt.show()"
""
"# Displaying learning curves:"
""
"plot_learning_curve(classifiers,"
"X_cat,"
"y_eli,"
"ylim=None,"
"cv=cv,"
"n_jobs=-1,"
"train_sizes=np.linspace(.1, 1.0, 10))"
""
"""""""# RandomizedSearchCV"
""
"### Let's get rid of overfitting, one of the easiest ways of doing it is tuning parameters for our estimators and regularize them. Thankfully sklearn is coming to help with useful tools for this case too! We going to use RandomizedSearchCV for this:"
""
"While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:"
""
"A budget can be chosen independent of the number of parameters and possible values."
""
"Adding parameters that do not influence the performance does not decrease efficiency."
""
"### I'm going to choose small amount of estimators and not many parameters to search for timing purposes but you'll see even that's useful!"
""""""""
""
"# Searching parameters for fine tuning:"
""
"for i in [randfclass,svc, adaclass]:"
""
"if i == svc:"
"parameters = {"
"'C': [1,3,9,27],"
"'tol': [1e-2, 1e-3, 1e-4],"
"'kernel': ['linear', 'rbf', 'sigmoid'],"
"'shrinking': [True, False]}"
""
"if i == randfclass:"
"parameters = {"
"'max_depth': [2, 3, 5],"
"'n_estimators': [50, 100, 150],"
"'criterion': ['gini', 'entropy'],"
"'bootstrap': [True, False],"
"}"
""
"if i == adaclass:"
"parameters = {"
"'estimator': [None, dectree],"
"'n_estimators': [50, 100, 150],"
"'algorithm': ['SAMME'],"
"'learning_rate': [0.8,1,1.2],"
"}"
""
""
"def hyperparameter_tune(base_model, parameters, n_iter, cv, X, y):"
""
"''' A function for optimizing mutliple classifiers'''"
""
"start_time = time.time()"
"optimal_model = RandomizedSearchCV(base_model,"
"param_distributions=parameters,"
"n_iter=n_iter,"
"cv=cv,"
"scoring = 'f1',"
"n_jobs=-1,"
"random_state=seed)"
""
"optimal_model.fit(X, y)"
""
""
"scores = cross_val_score(optimal_model, X, y, cv=cv,n_jobs=-1, scoring='f1')"
"stop_time = time.time()"
""
"print('====================')"
"print(f'Updated Parameters for {str(base_model.__class__.__name__)}')"
"print('Cross Val Mean: {:.3f}, Cross Val Stdev: {:.3f}'.format(scores.mean(), scores.std()))"
"print('Best Score: {:.3f}'.format(optimal_model.best_score_))"
"print('Best Parameters: {}'.format(optimal_model.best_params_))"
"print('Elapsed Time:', time.strftime('%H:%M:%S', time.gmtime(stop_time - start_time)))"
"print('====================')"
""
""
"return optimal_model.best_params_, optimal_model.best_score_"
"best_params, best_score = hyperparameter_tune(i, parameters, 20, cv, X_cat, y_eli)"
"i.set_params(**best_params)"
""
"""""""# Tuned Model Results"
""
"### Alright! As you can see even little bit tuned parameters added regularization to our models and increased the CV score for them. That's a good sign! Now we have three decent models to make predictions!"
""""""""
""
"# Checking binned models:"
""
"binn_models = model_check(X_cat, y_eli, classifiers, cv)"
"display(binn_models)"
""
"""""""# Dimension Reduction Using PCA"
""
"PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn, PCA is implemented as a transformer object that learns components in its fit method, and can be used on new data to project it on these components."
""
"Reducing dimensions is useful for bigger datasets because by transforming a large set of variables into a smaller one that still contains most of the information in the large set makes your modelling faster. This is not the case here since we have very small data but we still can use it for visualization which I find it cool..."
""""""""
""
"# Fitting PCA:"
""
"pca = PCA()"
"pca.fit(X_cat)"
"pca_samples = pca.transform(X_cat)"
""
"""""""## Explained Variance"
""
"### This graph shows that first 15 components explains more than 80% of the variance in the data and currently we have 46 of them. So it's safe to reduce some..."
""""""""
""
"# Explaining variance ratio:"
""
"fig, ax = plt.subplots(figsize=(14, 5))"
"plt.plot(range(pca.n_components_), pca.explained_variance_ratio_.cumsum(), linestyle='--', drawstyle='steps-mid', color=cust_palt[0],"
"label='Cumulative Explained Variance')"
"sns.barplot(x=np.arange(1, pca.n_components_ + 1), y=pca.explained_variance_ratio_, alpha=0.85, color=cust_palt[1],"
"label='Individual Explained Variance')"
""
"plt.ylabel('Explained Variance Ratio', fontsize = 14)"
"plt.xlabel('Number of Principal Components', fontsize = 14)"
"ax.set_title('Explained Variance', fontsize = 20)"
"plt.legend(loc='center right', fontsize = 13);"
"plt.show()"
""
"""""""## 5 Components"
""
"### We start with 5 components. It looks like these 5 explains half of the variance in our data."
""""""""
""
"# 5 Component PCA:"
""
"pca = PCA(5)"
"pca.fit(X_cat)"
"pca_samples = pca.transform(X_cat)"
""
"# Displaying 50% of the variance:"
""
"total_var = pca.explained_variance_ratio_.sum() * 100"
""
"labels = {str(i): f'PC {i+1}' for i in range(5)}"
"labels['color'] = 'condition'"
""
"fig = px.scatter_matrix("
"pca_samples,"
"color=y_eli,"
"dimensions=range(5),"
"labels=labels,"
"title=f'Total Explained Variance: {total_var:.2f}%',"
"opacity=0.8,"
"color_continuous_scale=cust_palt,"
")"
"fig.update_traces(diagonal_visible=False)"
"fig.show()"
""
"""""""## 3 Components"
""
"### More we reduce dimensions further we can visualize it better for our graphs. At 3D we can have this scatterplot, showing us some kind of meaningful clusters."
""""""""
""
"# 3 Component PCA:"
""
"pca = PCA(3)  # Project from 46 to 3 dimensions."
"matrix_3d = pca.fit_transform(X_cat)"
""
"# Displaying 3 components:"
""
"total_var = pca.explained_variance_ratio_.sum() * 100"
"fig = px.scatter_3d(x=matrix_3d[:, 0], y=matrix_3d[:, 1], z=matrix_3d[:, 2], color=y_eli, opacity=0.8,color_continuous_scale=cust_palt,"
"title=f'Total Explained Variance: {total_var:.2f}%',"
"labels = {'x':'Component 1', 'y':'Component 2','z':'Component 3'})"
"fig.show()"
""
"""""""### On 2D space we can still diverse the clusters according to our target variables. These two components explains almost one third of the variance..."""""""
""
"# 2 Component PCA:"
""
"pca = PCA(2)  # project from 46 to 2 dimensions"
"matrix_2d = pca.fit_transform(X_cat)"
""
"# Displaying 2 PCA:"
""
"total_var = pca.explained_variance_ratio_.sum() * 100"
"fig= plt.figure(figsize=(20, 12))"
"# Create a DataFrame for plotting"
"pca_2d_df = pd.DataFrame(matrix_2d, columns=['Component 1', 'Component 2'])"
"pca_2d_df['condition'] = y_eli.reset_index(drop=True) # Ensure index alignment"
""
"ax =sns.scatterplot(x='Component 1', y='Component 2', data=pca_2d_df ,palette=cust_palt[:2],"
"hue='condition', alpha=0.9, )"
"ax.set_title(f'Total Explained Variance: {total_var:.2f}%', fontsize = 20)"
"plt.xlabel('Component 1')"
"plt.ylabel('Component 2')"
""
"plt.show()"
""
"""""""> # Reduced Dimension Model Results"
""
"### These are pretty good results! We almost have same model metrics for most of the classifiers and even got some better regularization for some estimators!"
""""""""
""
"# Reduced Dimension Model Results"
""
"model_check(matrix_2d, y_eli, classifiers, cv)"
""
"""""""# Decision Regions"
""
"### With these contour plots we can see how the models decide on their predictions based on 2D data with confidence intervals. Looks cool!"
""""""""
""
"def prob_reg(X, y):"
""
"''' A function for displaying decision regions'''"
""
"from matplotlib.colors import ListedColormap"
"figure = plt.figure(figsize=(20, 40))"
"h = .02"
"i = 1"
""
"# preprocess dataset, split into training and test part"
"X_train, X_test, y_train, y_test = \"
"train_test_split(X, y, test_size=.2, random_state=42)"
""
"x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5"
"y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5"
"xx, yy = np.meshgrid(np.arange(x_min, x_max, h),"
"np.arange(y_min, y_max, h))"
""
"# Just plot the dataset first"
"cm = plt.cm.RdYlGn"
"cm_bright = ListedColormap(['#e00d14', '#3ca02c'])"
"ax = plt.subplot(5, 2, i)"
""
"# Iterate over classifiers"
"for clf in classifiers:"
"ax = plt.subplot(math.ceil(len(classifiers) / 2), 2, i)"
"clf.fit(X_train, y_train)"
"score = clf.score(X_test, y_test)"
""
"# Plot the decision boundary. For that, we will assign a color to each"
"# point in the mesh [x_min, x_max]x[y_min, y_max]."
"if hasattr(clf, 'decision_function'):"
"Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])"
"else:"
"Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]"
""
"# Put the result into a color plot"
"Z = Z.reshape(xx.shape)"
"ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)"
""
"# Plot the training points"
"g = ax.scatter(X_train[:, 0],"
"X_train[:, 1],"
"c=y_train,"
"cmap=cm_bright,"
"edgecolors='k')"
"# Plot the testing points"
"ax.scatter(X_test[:, 0],"
"X_test[:, 1],"
"c=y_test,"
"cmap=cm_bright,"
"edgecolors='k',"
"alpha=0.6)"
""
"ax.set_xlim(xx.min(), xx.max())"
"ax.set_ylim(yy.min(), yy.max())"
""
"ax.set_title(clf.__class__.__name__)"
""
"ax.set_xlabel('Component 1')"
"ax.set_ylabel('Component 2')"
"plt.legend(handles=g.legend_elements()[0],"
"labels=['No Disease', 'Has Disease'],"
"framealpha=0.3,"
"scatterpoints=1)"
""
"i += 1"
""
"plt.tight_layout()"
"plt.show()"
""
"prob_reg(matrix_2d, y_eli)"
""
"""""""# Confusion Matrix"
""
"### One last thing before we finish our sklearn journey I wanted to use another cool sklearn tool to show confusion matrices for each model. It's important for this case since we don't want our models to predict no disease on actually unhealty person or vice versa. It'd be very bad for patients in actual use. So we want less false positives and negatives but don't forget we still have some overfitted models so be careful about checking overfitted models like decision tree etc."
""""""""
""
"def conf_mat(X,y, classifiers):"
""
"''' A function for displaying confusion matrices'''"
""
"fig, axes = plt.subplots(4,2, figsize=(20,12))"
""
"axes = axes.flatten()"
""
"for ax, classifier in zip(axes, classifiers):"
"classifier.fit(X,y)"
"ConfusionMatrixDisplay.from_estimator(classifier, X, y,"
"values_format = 'n',"
"display_labels = ['No Disease', 'Diease'],"
"cmap='summer_r',ax=ax)"
"ax.set_title(f'{classifier.__class__.__name__}')"
"ax.grid(False)"
"plt.tight_layout()"
""
"# Displaying confusion matrix for each estimator:"
""
"conf_mat(X_cat, y_eli, classifiers)"
""
"""""""# Final Words"
""
"### Well this concludes my notebook. It was fun sklearn journey for me, I hope you had fun while reading it and thanks for taking a look."
""
"### Feel free to comment I'll try to answer it all and as usual if you liked my work please don't forget to vote, have good one all!"
""""""""
